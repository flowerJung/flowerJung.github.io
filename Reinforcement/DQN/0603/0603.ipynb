{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21203,"status":"ok","timestamp":1654226798168,"user":{"displayName":"박준희","userId":"11852115486633956458"},"user_tz":-540},"id":"4D8RblEmKSgx","outputId":"63189c4f-58b6-4dd0-ddb3-84f400110638"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1654226805213,"user":{"displayName":"박준희","userId":"11852115486633956458"},"user_tz":-540},"id":"v9LyT-ScLQ8f","outputId":"c5a358bd-4e4e-4c41-f9d9-20689b232bf9"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/project\n"]}],"source":["cd /content/drive/MyDrive/project"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":995,"status":"ok","timestamp":1654226851927,"user":{"displayName":"박준희","userId":"11852115486633956458"},"user_tz":-540},"id":"IymU0FioLQ6Q","outputId":"4d8fe199-fac7-4b30-e912-23d321a04559"},"outputs":[{"output_type":"stream","name":"stdout","text":["reward: -0.1 -0.5 10\n"]}],"source":["from string import ascii_uppercase\n","#from draw_utils import *\n","#from pyglet.gl import *\n","import numpy as np\n","import pandas as pd\n","import os\n","import copy\n","\n","\n","# reward\n","move_reward = -0.1\n","obs_reward = -0.5\n","goal_reward = 10\n","finish_reward = 20\n","print('reward:' , move_reward, obs_reward, goal_reward)\n","\n","local_path = os.path.abspath(os.path.join(os.path.dirname('__file__')))\n","\n","\n","class Simulator:\n","    def __init__(self):\n","        '''\n","        height : 그리드 높이\n","        width : 그리드 너비 \n","        inds : A ~ Q alphabet list\n","        '''\n","        # Load train data\n","        self.files = pd.read_csv(os.path.join(local_path, \"./data/factory_order_train.csv\")) #\"./data/factory_order_train.csv\"))\n","        self.height = 10\n","        self.width = 9\n","        self.inds = list(ascii_uppercase)[:17]\n","\n","    def set_box(self):\n","        '''\n","        아이템들이 있을 위치를 미리 정해놓고 그 위치 좌표들에 아이템이 들어올 수 있으므로 그리드에 100으로 표시한다.\n","        데이터 파일에서 이번 에피소드 아이템 정보를 받아 가져와야 할 아이템이 있는 좌표만 -100으로 표시한다.\n","        self.local_target에 에이전트가 이번에 방문해야할 좌표들을 저장한다.\n","        따라서 가져와야하는 아이템 좌표와 end point 좌표(처음 시작했던 좌표로 돌아와야하므로)가 들어가게 된다.\n","        '''\n","        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n","\n","        # 물건이 들어있을 수 있는 경우\n","        for box in box_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = 0\n","\n","        # 물건이 실제 들어있는 경우\n","        order_item = list(set(self.inds) & set(self.items))\n","        order_csv = box_data[box_data['item'].isin(order_item)]\n","        \n","        for order_box in order_csv.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = 200\n","            # local target에 가야 할 위치 좌표 넣기\n","            self.local_target.append(\n","                [getattr(order_box, \"row\"),\n","                 getattr(order_box, \"col\")]\n","                )\n","\n","        #self.local_target.append([9,4]) \n","        # 알파벳을 Grid에 넣어서 -> grid에 2Dconv 적용 가능\n","\n","    def set_obstacle(self):\n","        '''\n","        장애물이 있어야하는 위치는 미리 obstacles.csv에 정의되어 있다. 이 좌표들을 0으로 표시한다.\n","        '''\n","        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n","        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n","            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = 0\n","\n","    def reset(self, epi):\n","        '''\n","        reset()은 첫 스텝에서 사용되며 그리드에서 에이전트 위치가 start point에 있게 한다.\n","\n","        :param epi: episode, 에피소드 마다 가져와야 할 아이템 리스트를 불러올 때 사용\n","        :return: 초기셋팅 된 그리드\n","        :rtype: numpy.ndarray\n","        _____________________________________________________________________________________\n","        items : 이번 에피소드에서 가져와야하는 아이템들\n","        terminal_location : 현재 에이전트가 찾아가야하는 목적지\n","        local_target : 한 에피소드에서 찾아가야하는 아이템 좌표, 마지막 엔드 포인트 등의 위치좌표들\n","        actions: visualization을 위해 에이전트 action을 저장하는 리스트\n","        curloc : 현재 위치\n","        '''\n","\n","        # initial episode parameter setting\n","        self.epi = epi\n","        self.items = list(self.files.iloc[self.epi])[0]\n","        self.cumulative_reward = 0\n","        self.terminal_location = None\n","        self.local_target = []\n","        self.actions = []\n","        self.item_loc = False ## 수정\n","        \n","        # initial grid setting\n","        self.grid = np.ones((self.height, self.width), dtype=\"float16\")\n","\n","        # set information about the gridworld\n","        self.set_box()\n","        self.set_obstacle()\n","\n","        # start point를 grid에 표시\n","        self.curloc = [9, 4]\n","        self.grid[int(self.curloc[0])][int(self.curloc[1])] = 100\n","        \n","        self.done = False\n","        \n","        return self.grid\n","\n","    def apply_action(self, action, cur_x, cur_y):\n","        '''\n","        에이전트가 행한 action대로 현 에이전트의 위치좌표를 바꾼다.\n","        action은 discrete하며 4가지 up,down,left,right으로 정의된다.\n","        \n","        :param x: 에이전트의 현재 x 좌표\n","        :param y: 에이전트의 현재 y 좌표\n","        :return: action에 따라 변한 에이전트의 x 좌표, y 좌표\n","        :rtype: int, int\n","        '''\n","        new_x = cur_x\n","        new_y = cur_y\n","        # up\n","        if action == 0:\n","            new_x = cur_x - 1\n","        # down\n","        elif action == 1:\n","            new_x = cur_x + 1\n","        # left\n","        elif action == 2:\n","            new_y = cur_y - 1\n","        # right\n","        else:\n","            new_y = cur_y + 1\n","\n","        return int(new_x), int(new_y)\n","\n","\n","    def get_reward(self, new_x, new_y, out_of_boundary):\n","        '''\n","        get_reward함수는 리워드를 계산하는 함수이며, 상황에 따라 에이전트가 action을 옳게 했는지 판단하는 지표가 된다.\n","\n","        :param new_x: action에 따른 에이전트 새로운 위치좌표 x\n","        :param new_y: action에 따른 에이전트 새로운 위치좌표 y\n","        :param out_of_boundary: 에이전트 위치가 그리드 밖이 되지 않도록 제한\n","        :return: action에 따른 리워드\n","        :rtype: float\n","        '''\n","\n","        # 바깥으로 나가는 경우\n","        if any(out_of_boundary):\n","            reward = obs_reward\n","                       \n","        else:\n","            # 장애물에 부딪히는 경우 \n","            if self.grid[new_x][new_y] == 0:\n","                reward = obs_reward  \n","\n","            # 현재 목표에 도달한 경우\n","            elif [new_x, new_y] in self.terminal_location:\n","                if [new_x, new_y] == [9, 4]:\n","                    reward = finish_reward\n","                else:\n","                    reward = goal_reward\n","\n","            # 그냥 움직이는 경우 \n","            else:\n","                reward = move_reward\n","\n","        return reward\n","\n","    def step(self, action):\n","        ''' \n","        에이전트의 action에 따라 step을 진행한다.\n","        action에 따라 에이전트 위치를 변환하고, action에 대해 리워드를 받고, 어느 상황에 에피소드가 종료되어야 하는지 등을 판단한다.\n","        에이전트가 endpoint에 도착하면 gif로 에피소드에서 에이전트의 행동이 저장된다.\n","\n","        :param action: 에이전트 행동\n","        :return:\n","            grid, 그리드\n","            reward, 리워드\n","            cumulative_reward, 누적 리워드\n","            done, 종료 여부\n","            goal_ob_reward, goal까지 아이템을 모두 가지고 돌아오는 finish율 계산을 위한 파라미터\n","\n","        :rtype: numpy.ndarray, float, float, bool, bool/str\n","\n","        (Hint : 시작 위치 (9,4)에서 up말고 다른 action은 전부 장애물이므로 action을 고정하는 것이 좋음)\n","        '''\n","\n","        self.terminal_location = copy.deepcopy(self.local_target)\n","        cur_x,cur_y = self.curloc\n","        self.actions.append((cur_x, cur_y))\n","\n","        goal_ob_reward = False\n","        \n","        new_x, new_y = self.apply_action(action, cur_x, cur_y)\n","\n","        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]\n","\n","        # 바깥으로 나가는 경우 종료\n","        if any(out_of_boundary):\n","            pass\n","            #self.done = True\n","            #goal_ob_reward = True\n","        else:\n","            # 장애물에 부딪히는 경우 종료\n","            if self.grid[new_x][new_y] == 0:\n","                pass\n","                #self.done = True\n","                #goal_ob_reward = True\n","\n","            # 현재 목표에 도달한 경우\n","            elif [new_x, new_y] in self.terminal_location:\n","\n","                # end point 일 때\n","                if [new_x, new_y] == [9,4]:\n","                    \n","                    self.done = True\n","                    self.local_target.remove([new_x, new_y])\n","                \n","                # item 일때\n","                else:\n","                    self.local_target.remove([new_x, new_y])\n","                    if not self.local_target:\n","                        self.local_target.append([9,4])\n","                        self.grid[9][4] = 200\n","                \n","                if self.item_loc: #저번에가 item 이었던 자리었으면\n","                    self.grid[cur_x][cur_y] = 0\n","                    self.grid[new_x][new_y] = 100\n","                else:\n","                    self.grid[cur_x][cur_y] = 1\n","                    self.grid[new_x][new_y] = 100\n","\n","                goal_ob_reward = True\n","                self.item_loc=True\n","                \n","                self.curloc = [new_x, new_y]\n","            else:\n","                # 그냥 움직이는 경우\n","                if self.item_loc:\n","                    self.grid[cur_x][cur_y] = 0\n","                    self.grid[new_x][new_y] = 100\n","                    self.item_loc = False\n","\n","                else:\n","                    self.grid[cur_x][cur_y] = 1\n","                    self.grid[new_x][new_y] = 100\n","                    \n","                self.curloc = [new_x,new_y]\n","                \n","        reward = self.get_reward(new_x, new_y, out_of_boundary)\n","        self.cumulative_reward += reward\n","\n","        # if self.done == True:\n","        #     if [new_x, new_y] == [9, 4]:\n","        #         if self.terminal_location[0] == [9, 4]:\n","        #                 # 완료되면 GIFS 저장\n","\n","        #             if len(self.actions) < 500:\n","        #                 print(f'500번 안에 들어왔다! : {len(self.actions)}')\n","        #                 goal_ob_reward = 'finish'\n","        #                 height = 10\n","        #                 width = 9 \n","        #                 display = Display(visible=False, size=(width, height))\n","        #                 display.start()\n","\n","        #                 start_point = (9, 4)\n","        #                 unit = 50\n","        #                 screen_height = height * unit\n","        #                 screen_width = width * unit\n","        #                 log_path = \"./logsdqn\"\n","        #                 data_path = \"./data\"\n","        #                 render_cls = Render(screen_width, screen_height, unit, start_point, data_path, log_path)\n","        #                 for idx, new_pos in enumerate(self.actions):\n","        #                     render_cls.update_movement(new_pos, idx+1)\n","\n","        #                 render_cls.save_gif(self.epi)\n","        #                 render_cls.viewer.close()\n","        #                 display.stop()\n","        #             else:\n","        #                 print(len(self.actions))\n","        \n","        \n","        \n","        \n","        return self.grid, reward, self.cumulative_reward, self.done, goal_ob_reward"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NO0EB_asLQ4A"},"outputs":[],"source":["import collections\n","import pdb\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","#from Sim import *\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uacNA-E0LQ1q"},"outputs":[],"source":["## Replay buffer\n","\n","class ReplayBuffer():\n","    def __init__(self):\n","        self.buffer = collections.deque(maxlen=buffer_limit)\n","\n","    def put(self, transition):\n","        self.buffer.append(transition)\n","\n","    def sample(self, n):\n","        mini_batch = random.sample(self.buffer, n)     #sample 메서드 : buffer 중에 n 개만 뽑음\n","        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n","                                                        # state 값, action 값, reward 값, 다음 state 값, done_mask 값\n","                                                        # done_mask : 종료 상태의 Value 값을 마스킹해줍니다\n","\n","        for transition in mini_batch: #우리가 뽑은 n 개의 미니배치들 하나씩에서\n","            s, a, r, s_prime, done_mask = transition #\n","\n","            # 이로 보아 하나의 sample 에는 s,a,s_prime,done_mask 값이 담김을 확인할 수 있다\n","            s_lst.append(s)\n","            a_lst.append([a]) # 자료형이 list 임을 확인 가능, 여러개의 action 이 담겨있을 것이라 추측\n","            r_lst.append([r])\n","            s_prime_lst.append(s_prime)\n","            done_mask_lst.append([done_mask])\n","\n","        return torch.tensor(s_lst, dtype=torch. float), torch.tensor(a_lst), torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch. float), torch.tensor(done_mask_lst)\n","        # s_lst, s_prime_lst 는 타입을 바꿔주었다. 데이터가 int 였기 때문에\n","        # 나머지는 tensor화\n","\n","    def size(self):\n","        return len(self.buffer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nb6b91YnLQzq"},"outputs":[],"source":["class QnetCNN(nn.Module):\n","    def __init__(self):\n","        super(QnetCNN, self).__init__()\n","        \n","        \n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n","            nn.ReLU())\n","\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(6, 16, kernel_size=3),\n","            nn.ReLU())\n","        \n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(16, 32, kernel_size=3),\n","            nn.ReLU())\n","    \n","        #Flatten\n","        self.flatten = nn.Flatten()\n","\n","        self.fc1 = nn.Linear(64, 16)\n","        self.fc2 = nn.Linear(16, 4)\n","        \n","    def forward(self,x):\n","        x = torch.from_numpy(np.asarray(x)).float()\n","        x = torch.reshape(x, (-1, 1, 10, 9))\n","        \n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.fc2(x)# 좌우상하 4개의 값을 반환\n","        return x\n","\n","    def sample_action(self, obs, epsilon):\n","        out = self.forward(obs) \n","        coin = random.random() \n","        \n","\n","        if coin < epsilon: \n","            return random.randint(0,3)\n","        else:\n","            return out.argmax().item() "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCoNRH8TMUXN"},"outputs":[],"source":["def train(q, q_target, memory, optimizer): \n","    for i in range(10):\n","        s, a, r, s_prime, done_mask = memory.sample(batch_size) #32개를 버퍼에서 뽑아 모아 놓은 s,a,r,s_prime,done_mask\n","        q_out = q(s)                                            # s 값으로 다음 각 action 값들의 value 값 반환\n","        q_a = q_out.gather(1,a)                      #선택한 액션값들의 q(s,a) 반환\n","        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)  # 다음 state의 각 q(s,a) 값 반환\n","        target = r + gamma * max_q_prime * done_mask # 배열 맞춰주기, 쓰러진 경우는 제거\n","        \n","        loss = F.smooth_l1_loss(q_a, target)                    # DQN 의 손실함수 계산 L1 유클리드\n","\n","        optimizer.zero_grad()                                   # optimizer 의 모든 parameter 를 0으로 변환\n","        loss.backward()                                         # loss 에 대한 gradient 계산\n","        optimizer.step()                                        # 손실값을 바탕으로 Qnet 의 파라미터 업데이트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"osfv6JADMUU-"},"outputs":[],"source":["def model_save(model_dict, epi):\n","    PATH = './weights/'\n","    torch.save({\n","            'model': model_dict,\n","            'epi': epi,\n","            }, PATH + 'all_0602_itemall_full_from1v1.tar')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cKbmSER-LQxO"},"outputs":[],"source":["import time\n","from tqdm import tqdm\n","\n","learning_rate = 0.0001\n","gamma = 0.99\n","buffer_limit  = 100000\n","batch_size = 128\n","\n","\n","USE_CUDA = torch.cuda.is_available()\n","print(USE_CUDA)\n","device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n","\n","def main():\n","\n","    env = Simulator()\n","    files = pd.read_csv(\"./data/factory_order_train.csv\")\n","\n","    q = QnetCNN()\n","    q_target = QnetCNN()\n","\n","\n","    # loss 값을 바탕으로 업데이트할 비율 (q_target 말고 q 만 업데이트) \n","    ## 모델 load 하기 위해 위치 변경  \n","    optimizer = optim.Adam(q.parameters(), lr = learning_rate) \n","    memory = ReplayBuffer()\n","\n","    # 체크포인트 로드 #######################\n","    PATH = './weights/'\n","    checkpoint = torch.load(PATH+'all_0602_item7_full_from1v1.tar')\n","    q.load_state_dict(checkpoint['model'])\n","    q.eval()\n","    \n","    # 현재 Qnet 의 파라미터를 q_target 에 load\n","    q_target.load_state_dict(q.state_dict())      \n","    \n","\n","    \n","\n","    print_interval = 100\n","    score = 0.0\n","    \n","\n","\n","    # 마지막 보상 출력을 위한 변수\n","    cnt = 0\n","\n","    for epi in tqdm(range(300000)):\n","        #time.sleep(0.001)\n","        if epi%1000 == 0:\n","            model_save(q.state_dict(), epi)\n","            \n","        epsilon = max(0.01, 0.1 - 0.0005 * (epi//100))\n","        \n","        masking = False\n","            \n","        ep = epi%39999\n","\n","        s = env.reset(ep)\n","        obs = np.asarray(s, dtype=np.float32) \n","\n","        a_step = 0\n","        done = False \n","        first = True\n","        while not done: \n","            \n","            if first:\n","                a = 0\n","                first = False\n","                \n","            elif obs[9][4] == 50:\n","                a = 0\n","                \n","            else:\n","                a = q.sample_action(torch.from_numpy(obs).float(), epsilon) \n","            \n","            s_prime, r, cumul, done, goal_reward = env.step(a)\n","            \n","            if done:\n","                masking = True\n","            \n","            if masking : # 끝났으면 train 안함\n","                cnt += 1\n","                print(f'지금 다 먹고 {cnt}번째 도착완료 총 보상은 : {cumul} ep: {ep} step: {a_step} ')\n","                masking = False\n","            \n","            if a_step == 200:\n","                done = True\n","                goal_reward = True\n","            \n","            array = s_prime\n","            s_prime = np.asarray(s_prime, dtype=np.float32)\n","             \n","            done_mask = 0.0 if done else 1.0\n","\n","            memory.put((obs, a, r/100, s_prime, done_mask))\n","\n","            \n","            obs = s_prime \n","            score += r\n","        \n","\n","            a_step +=1 # a 의 시행횟수\n","\n"," \n","\n","        if memory.size() > 2000: \n","            train(q, q_target, memory, optimizer)\n","        \n","\n","\n","        if epi%print_interval==0 and epi != 0: \n","            action = ['↑', '↓', '←', '→'][a]\n","            print(f'epi = {ep}\\n 마지막 상태 :\\n{array}\\n 이 때 한 행동 : {action}\\n 마지막 보상 : {r}\\n 총 받은 보상 :{cumul}\\n end : {done}\\n clear : {goal_reward}')\n","           # env.now_state()\n","            \n","            q_target.load_state_dict(q.state_dict()) #q_target 지금걸로 업데이트\n","            print(f\"episode :{ep}, score = {score/print_interval}, n_buffer :{memory.size()} , eps : {epsilon*100}\")\n","            score = 0.0\n","\n","main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVVAiikyLQtN"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wqiSRV5mLQmg"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"name":"alllllllllllllllllll2.ipynb","provenance":[],"machine_shape":"hm","collapsed_sections":[],"authorship_tag":"ABX9TyMxTSnmDZETHXpPwps5NvPS"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}